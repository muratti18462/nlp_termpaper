{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from datasets import DatasetDict,Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:06:47.961002Z",
     "iopub.status.busy": "2025-08-07T19:06:47.960164Z",
     "iopub.status.idle": "2025-08-07T19:06:48.463321Z",
     "shell.execute_reply": "2025-08-07T19:06:48.462541Z",
     "shell.execute_reply.started": "2025-08-07T19:06:47.960973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.txt\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:06:48.464729Z",
     "iopub.status.busy": "2025-08-07T19:06:48.464468Z",
     "iopub.status.idle": "2025-08-07T19:06:48.492607Z",
     "shell.execute_reply": "2025-08-07T19:06:48.491925Z",
     "shell.execute_reply.started": "2025-08-07T19:06:48.464702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"A\"] = df[\"A\"].astype(str)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df = df[~df[\"A\"].str.contains(r',|\\.|\\?', regex=True ,na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:06:48.782349Z",
     "iopub.status.busy": "2025-08-07T19:06:48.781830Z",
     "iopub.status.idle": "2025-08-07T19:06:48.961631Z",
     "shell.execute_reply": "2025-08-07T19:06:48.960800Z",
     "shell.execute_reply.started": "2025-08-07T19:06:48.782325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mappp ={\"I\":\"I-CHEMICAL\",\"B\":\"B-CHEMICAL\",\"O\":\"O\"}\n",
    "df[\"O\"] =  df[\"O\"].map(mappp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:06:49.307615Z",
     "iopub.status.busy": "2025-08-07T19:06:49.307360Z",
     "iopub.status.idle": "2025-08-07T19:06:49.333650Z",
     "shell.execute_reply": "2025-08-07T19:06:49.332805Z",
     "shell.execute_reply.started": "2025-08-07T19:06:49.307593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:06:49.335523Z",
     "iopub.status.busy": "2025-08-07T19:06:49.334824Z",
     "iopub.status.idle": "2025-08-07T19:06:56.037037Z",
     "shell.execute_reply": "2025-08-07T19:06:56.036460Z",
     "shell.execute_reply.started": "2025-08-07T19:06:49.335496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def create_combined_dataset(df, min_rows=5, max_rows=10):\n",
    "    \n",
    "    new_data = []\n",
    "    current_index = 0\n",
    "    total_rows = len(df)\n",
    "\n",
    "    while current_index < total_rows:\n",
    "\n",
    "        num_rows_to_combine = random.randint(min_rows, max_rows)\n",
    "        \n",
    "        end_index = min(current_index + num_rows_to_combine, total_rows)\n",
    "        slice_df = df.iloc[current_index:end_index]\n",
    "        \n",
    "        combined_text = \" \".join(slice_df['A'].tolist())\n",
    "        merged_labels = \" \".join(slice_df['O'].tolist())\n",
    "        \n",
    "        new_data.append({'text': combined_text, 'labels': merged_labels})\n",
    "        \n",
    "        current_index = end_index\n",
    "\n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "df_new = create_combined_dataset(df, min_rows=5, max_rows=10)\n",
    "df_new[\"id\"] = df_new.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:06:56.038358Z",
     "iopub.status.busy": "2025-08-07T19:06:56.038142Z",
     "iopub.status.idle": "2025-08-07T19:06:56.046668Z",
     "shell.execute_reply": "2025-08-07T19:06:56.045971Z",
     "shell.execute_reply.started": "2025-08-07T19:06:56.038337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:30.380728Z",
     "iopub.status.busy": "2025-08-07T19:07:30.380254Z",
     "iopub.status.idle": "2025-08-07T19:07:30.384706Z",
     "shell.execute_reply": "2025-08-07T19:07:30.383975Z",
     "shell.execute_reply.started": "2025-08-07T19:07:30.380710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:31.114299Z",
     "iopub.status.busy": "2025-08-07T19:07:31.114027Z",
     "iopub.status.idle": "2025-08-07T19:07:31.129142Z",
     "shell.execute_reply": "2025-08-07T19:07:31.128452Z",
     "shell.execute_reply.started": "2025-08-07T19:07:31.114277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_new.rename(columns={'text': 'material', 'labels': 'product'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:31.131402Z",
     "iopub.status.busy": "2025-08-07T19:07:31.130985Z",
     "iopub.status.idle": "2025-08-07T19:07:31.148980Z",
     "shell.execute_reply": "2025-08-07T19:07:31.148353Z",
     "shell.execute_reply.started": "2025-08-07T19:07:31.131383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id= {\n",
    "    \"B-CHEMICAL\": 0,\n",
    "    \"I-CHEMICAL\": 1,\n",
    "    \"O\": 2\n",
    "  }\n",
    "tag_values = list(label2id.keys())\n",
    "\n",
    "id2label = {tag: idx for idx, tag in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:37.421842Z",
     "iopub.status.busy": "2025-08-07T19:07:37.421110Z",
     "iopub.status.idle": "2025-08-07T19:07:43.352133Z",
     "shell.execute_reply": "2025-08-07T19:07:43.351284Z",
     "shell.execute_reply.started": "2025-08-07T19:07:37.421815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_tokens_and_tags(material, product):\n",
    "    tokens_raw = [mat.strip(\",\") for mat in material.split(\" \")]\n",
    "    tokens = [tam.strip(\",\") for tam in product.split(\" \")]\n",
    "    tokens = [label2id[tam] for tam in tokens]\n",
    "\n",
    "    return tokens_raw, tokens\n",
    "\n",
    "formatted_data_chem = []\n",
    "for i, row in df_new.iterrows():\n",
    "     material = row['material']\n",
    "     tokens = row['product']\n",
    "     tokens_raw, tokens = create_tokens_and_tags(material, tokens)\n",
    "     formatted_data_chem.append({\n",
    "     \"id\": str(i),\n",
    "     \"tokens\": tokens_raw, \n",
    "     \"ner_tags\": tokens  \n",
    "    })\n",
    "\n",
    "formatted_df_chem = pd.DataFrame(formatted_data_chem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:43.353606Z",
     "iopub.status.busy": "2025-08-07T19:07:43.353328Z",
     "iopub.status.idle": "2025-08-07T19:07:43.361989Z",
     "shell.execute_reply": "2025-08-07T19:07:43.361440Z",
     "shell.execute_reply.started": "2025-08-07T19:07:43.353588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "formatted_df_chem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df_chem = formatted_df_chem[~formatted_df_chem['ner_tags'].apply(lambda x: all(val == 2 for val in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:43.362731Z",
     "iopub.status.busy": "2025-08-07T19:07:43.362564Z",
     "iopub.status.idle": "2025-08-07T19:07:44.087997Z",
     "shell.execute_reply": "2025-08-07T19:07:44.087393Z",
     "shell.execute_reply.started": "2025-08-07T19:07:43.362718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "formatted_df_chem = formatted_df_chem.sample(frac=0.8).reset_index(drop=True)\n",
    "\n",
    "ds = Dataset.from_dict({'id': formatted_df_chem['id'].tolist(),\n",
    "    'tokens': formatted_df_chem['tokens'].tolist(),\n",
    "    'ner_tags': formatted_df_chem['ner_tags'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:44.089650Z",
     "iopub.status.busy": "2025-08-07T19:07:44.089395Z",
     "iopub.status.idle": "2025-08-07T19:07:44.139378Z",
     "shell.execute_reply": "2025-08-07T19:07:44.138688Z",
     "shell.execute_reply.started": "2025-08-07T19:07:44.089632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_testvalid = ds.train_test_split(test_size=0.2)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.6)\n",
    "ds = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T19:07:44.140294Z",
     "iopub.status.busy": "2025-08-07T19:07:44.140075Z",
     "iopub.status.idle": "2025-08-07T19:07:44.144722Z",
     "shell.execute_reply": "2025-08-07T19:07:44.144054Z",
     "shell.execute_reply.started": "2025-08-07T19:07:44.140272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "def tokenize_and_align_labels(samples):\n",
    "    tokenized_inputs = tokenizer(samples[\"tokens\"],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for idx, label in enumerate(samples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        prev_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids: # set special tokens to -100\n",
    "            if word_idx is None or word_idx == prev_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ds = ds.map(tokenize_and_align_labels,\n",
    "                       batched=True,\n",
    "                       remove_columns=\n",
    "                        [\n",
    "                            'ner_tags',\n",
    "                            'tokens'\n",
    "                        ]\n",
    "                    )\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "tag_values = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                             \n",
    "    num_labels=len(tag_values),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        label_list[p] for prediction, label in zip(predictions, labels)\n",
    "        for p, l in zip(prediction, label) if l != -100\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        label_list[l] for prediction, label in zip(predictions, labels)\n",
    "        for p, l in zip(prediction, label) if l != -100\n",
    "    ]\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1\": report[\"macro avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "config = {\n",
    "    \"MODEL_NAME\": \"debertabase_term_paper\",\n",
    "    \"HUGGINGFACE_API_KEY\": \"\"\n",
    "    \"REPORTS_TO\": \"tensorboard\"\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "output_dir=config[\"MODEL_NAME\"],\n",
    "num_train_epochs=4,\n",
    "auto_find_batch_size=True,\n",
    "gradient_accumulation_steps=4,\n",
    "eval_accumulation_steps=2,\n",
    "eval_strategy=\"epoch\",\n",
    "logging_strategy=\"steps\",\n",
    "logging_steps=100, \n",
    "warmup_steps=500,\n",
    "logging_first_step=True, \n",
    "learning_rate=5e-4,\n",
    "report_to=config[\"REPORTS_TO\"],\n",
    "weight_decay=0.001,\n",
    "disable_tqdm=False,\n",
    "fp16=True,\n",
    "group_by_length=True,\n",
    "push_to_hub=False,\n",
    "hub_private_repo=True,\n",
    "hub_token=config[\"HUGGINGFACE_API_KEY\"],\n",
    "save_strategy=\"no\" \n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=encoded_ds[\"train\"],\n",
    "    eval_dataset=encoded_ds[\"valid\"],\n",
    "    data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertConfig, pipeline\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muratti18462/debertabase_term_paper\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"muratti18462/debertabase_term_paper\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(type='pandas')\n",
    "df = ds['test'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = model.config.label2id\n",
    "id2label = model.config.id2label\n",
    "\n",
    "def get_predicted_labels(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    outputs = ner_pipeline(text)\n",
    "\n",
    "    predicted_labels = [\"O\"] * len(tokens)\n",
    "    \n",
    "    for entity in outputs:\n",
    "        word = entity['word']\n",
    "        entity_label = entity['entity_group']\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        \n",
    "        entity_tokens = tokenizer.tokenize(word)\n",
    "        for i, token in enumerate(tokens):\n",
    "            if word.lower() in token.lower():\n",
    "                predicted_labels[i] = entity_label\n",
    "                break\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tokens = row[\"tokens\"]\n",
    "    gold_ids = row[\"ner_tags\"]\n",
    "    gold_labels = [id2label[id] for id in gold_ids]\n",
    "\n",
    "    preds = get_predicted_labels(tokens)\n",
    "\n",
    "    true_labels.append(gold_labels)\n",
    "    predicted_labels.append(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Precision:\", precision_score(true_labels, predicted_labels))\n",
    "print(\"Recall:\", recall_score(true_labels, predicted_labels))\n",
    "print(\"F1-score:\", f1_score(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({'id': formatted_df_chem['id'].tolist(),\n",
    "    'tokens': formatted_df_chem['tokens'].tolist(),\n",
    "    'ner_tags': formatted_df_chem['ner_tags'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_testEval = ds.train_test_split(test_size=0.01)\n",
    "ds = DatasetDict({\n",
    "    \"train\" : train_testEval[\"train\"],\n",
    "    \"test\": train_testEval[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentences = formatted_df_chem[\"tokens\"].to_list()\n",
    "labels = formatted_df_chem[\"ner_tags\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flat_tokens = [token for sentence in sentences for token in sentence]\n",
    "flat_labels = [label for sentence_labels in labels for label in sentence_labels]\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", lowercase=True)\n",
    "X = vectorizer.fit_transform(flat_tokens)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel=\"linear\", C=1.0)\n",
    "svm.fit(X, flat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('my_svm_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(svm, f) \n",
    "\n",
    "with open('my_svm_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = pd.read_pickle(\"my_svm_classifier.pkl\")\n",
    "vectorizer = pd.read_pickle(\"my_svm_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(type='pandas')\n",
    "\n",
    "df_train = ds['train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(type='pandas')\n",
    "\n",
    "df = ds['test'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predicted_tags\"] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    X_new = vectorizer.transform(row[\"tokens\"])\n",
    "    predictions = svm.predict(X_new)\n",
    "    \n",
    "    df.at[i, \"predicted_tags\"] = predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"ner_tags\"].to_list()\n",
    "predicted_labels = df[\"predicted_tags\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [[id2label[int(tag)] for tag in seq] for seq in df[\"ner_tags\"]]\n",
    "\n",
    "predicted_labels = [[id2label[int(tag)] for tag in seq] for seq in df[\"predicted_tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Precision:\", precision_score(true_labels, predicted_labels))\n",
    "print(\"Recall:\", recall_score(true_labels, predicted_labels))\n",
    "print(\"F1-score:\", f1_score(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13289915,
     "sourceId": 106949,
     "sourceType": "competition"
    },
    {
     "datasetId": 3156628,
     "sourceId": 5477008,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
